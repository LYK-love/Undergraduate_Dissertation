import json
import random


from Project.code.CommentSimilarity.SentenceCloneType import SentenceCloneType
from Project.code.data_process.deduplicate import deduplicate
from Project.code.data_process.generate_cleaned_dataset import generate_cleaned_dataset_and_save
from Project.code.data_process.label_correction import label_correction
from Project.code.utils.Manually_label import write_label_to_dataset
from Project.code.utils.ZeroShot import add_classification_property_using_model
from Project.code.utils.sentence_clone_detection import detect_sentence_clone


# def read_json_file(filename, num_lines):
#     with open(filename, 'r') as file:
#         lines = []
#         for i, line in enumerate(file):
#             if i >= num_lines:
#                 break
#             json_obj = json.loads(line)
#             lines.append(json_obj)
#         return lines

def read_json_file(file_path):
    '''

    :param file_path:
    :return: list(dict), each dict is a json object
    '''
    data = []
    with open(file_path, 'r') as file:
        for line in file:
            json_obj = json.loads(line)
            data.append(json_obj)
    return data

def get_line_cnt(file_path):
    '''

    :param file_path:
    :return: list(dict), each dict is a json object
    '''
    line_cnt = 0
    with open(file_path, 'r') as file:
        for line in file:
            line_cnt+=1
    return line_cnt

def get_dataset_ratio(training_dataset_path, test_dataset_path):
    '''
    Get the ratio of training_dataset  and test_dataset, should be 4 : 1 ( return 0.8)
    Args:
        training_dataset_path:
        test_dataset_path:

    Returns:

    '''
    size_of_training_dataset = get_line_cnt(training_dataset_path)
    size_of_test_dataset = get_line_cnt(test_dataset_path)

    size_of_total = size_of_training_dataset + size_of_test_dataset


    ratio = size_of_training_dataset / size_of_total
    return ratio

def read_json_file_for_given_line(file_path, line_count):
    '''

    :param file_path:
    :return: list(dict), each dict is a json object
    '''
    data = []

    with open(file_path, "r") as file:
        for i, line in enumerate(file):
            if i >= line_count:
                break
            json_obj = json.loads(line)
            data.append(json_obj)
    return data

def rewrite_jsonl_file(input_file_path, output_file_path, num_lines):
    '''
    å°†åŸå§‹çš„ JSONL æ–‡ä»¶çš„å‰ M è¡Œé‡æ–°å†™å…¥åˆ°åŒä¸€æ–‡ä»¶ä¸­ï¼š
    Args:
        input_file_path:
        output_file_path:
        num_lines:

    Returns:

    '''
    new_data = read_json_file_for_given_line(input_file_path, num_lines)
    save_jsonl_file(new_data,output_file_path)



# ç»™æ¯ä¸ªJSONå¯¹è±¡æ·»åŠ  classification å±æ€§, è¿™ä¸ªæ–¹æ³•ä»…ä»…ç”¨åšæ¼”ç¤º. å®é™…ä½¿ç”¨çš„æ˜¯CPCçš„ä»£ç .
def add_classification_property(json_obj_list):
    '''
    :param json_obj_list: list(dict), each dict is a json object
    :return:
    '''
    # ---------------------

    use_model = False
    if use_model:
        # Use ZeroShot to implement CPC's comment classification. Because it's extremely slow, so during the test/dubug procudure,
        # we don;t use it and use a mock method alternatively.
        # json_obj_list = json_obj_list[:10]
        json_obj_list = add_classification_property_using_model(json_obj_list)
        # ---------------------
    else:
        for json_obj in json_obj_list:
            classification = {
                'what': random.choices([0, 1], weights=[0.8, 0.2])[0],
                'why': random.choices([0, 1], weights=[0.95, 0.05])[0],
                'how-it-is-done': random.choices([0, 1], weights=[0.6, 0.4])[0],
                'how-to-use': random.choices([0, 1], weights=[0.85, 0.15])[0],
                'property': random.choices([0, 1], weights=[0.2, 0.8])[0]
            }

            # ä¸å…è®¸å…¨1æˆ–å…¨0çš„æƒ…å†µ
            if all(value == 0 for value in classification.values()):
                classification['what'] = 1
            elif all(value == 1 for value in classification.values()):
                classification['what'] = 0
                classification['why'] = 0
                classification['how-it-is-done'] = 0
                classification['how-to-use'] = 0
                classification['property'] = 1

            json_obj["classification"] = classification

    return json_obj_list

def add_sentence_level_clone_type(json_obj_list):
    '''
    add "clone_type" attribute to each json_obj.
    "clone_type":
    {
        "textual": 0
        "lexical": 1
        "syntactic": 0
        "semantic": 0
    }

    :param json_obj_list:
    :return:
    '''
    for json_obj in json_obj_list:


        is_type_1_clone = 0
        is_type_2_clone = 0
        is_type_3_clone = 0
        is_type_4_clone = 0

        is_clone = False
        is_clone = detect_sentence_clone(json_obj, SentenceCloneType(1))
        if is_clone:
            is_type_1_clone = 1
        else:
            is_clone = detect_sentence_clone(json_obj, SentenceCloneType(2))
            if is_clone:
                is_type_2_clone = 1
            else:
                is_clone = detect_sentence_clone(json_obj, SentenceCloneType(3))
                if is_clone:
                    is_type_3_clone = 1
                else:
                    is_type_4_clone = random.randint(0, 1)

        sentence_clone_type = {
            "textual": is_type_1_clone,
            "lexical": is_type_2_clone,
            "syntactic": is_type_3_clone,
            "semantic": is_type_4_clone
        }

        json_obj["clone_type"] = sentence_clone_type

    return json_obj_list

def look_classification(processed_data_file_path):
    processed_data = read_json_file(processed_data_file_path)
    for single_sample in processed_data:
        classification_info = single_sample['classification'] # {what: 0, how: 1, why: 0}
        print(single_sample)

        for key,value  in classification_info.items():
            print(f'{key}: {value}; ', end="")
        print()

def save_jsonl_file(data, file_path):
    with open(file_path, 'w') as file:
        for obj in data:
            json_line = json.dumps(obj)
            file.write(json_line + '\n')


def modify_and_clean_dataset(original_dataset_dir, original_update_dataset_dir, cleaned_dataset_dir, id_deleted_dir_path, sample_id_files_dir_path ):
    '''
    original_update_dataset_dir åªæ˜¯è¢«ç”¨ä½œlabel_correctionï¼Œä¸ä¼šä½œä¸ºæ•°æ®é›†çš„ä¸€éƒ¨åˆ†ã€‚
    Args:
        original_dataset_dir:
        original_update_dataset_dir:
        cleaned_dataset_dir:
        id_deleted_dir_path:
        sample_id_files_dir_path:

    Returns:

    '''
    deduplicate(original_dataset_dir,id_deleted_dir_path)#å‘{id_deleted_dir_path}ç›®å½•ä¸‹è¾“å‡ºæ–‡ä»¶
    label_correction(original_update_dataset_dir, sample_id_files_dir_path)# å‘{sample_id_files_dir_path}ç›®å½•ä¸‹è¾“å‡ºæ–‡ä»¶
    generate_cleaned_dataset_and_save(original_dataset_dir, cleaned_dataset_dir, id_deleted_dir_path, sample_id_files_dir_path)#åˆ©ç”¨{id_deleted_dir_path}, {sample_id_files_dir_path}ç›®å½•ä¸‹çš„æ–‡ä»¶ä½œä¸ºè§„åˆ™æ¥è¿‡æ»¤æ•°æ®é›†
    print(f"Dataset Cleaning End. Cleaned dataset is saved in dir {cleaned_dataset_dir}")



def process_sentence_and_save(file_path, classified_file_path, clone_detected_file_path):
    data = read_json_file(file_path)

    #æ³¨é‡Šçš„è¯­å¥åˆ†ç±»
    classified_data = add_classification_property(data)
    save_jsonl_file(classified_data, classified_file_path)
    print(f'Classification end. Save data to {classified_file_path}')

    # æ³¨é‡Šçš„è¯­å¥çš„å…‹éš†æ£€æµ‹
    clone_detected_data = add_sentence_level_clone_type(classified_data)
    save_jsonl_file(clone_detected_data, clone_detected_file_path)
    print(f'Clone detection end. Save data to {clone_detected_file_path}')
    print("Sentence Processing End.")

def process_sentence(file_path):
    data = read_json_file(file_path)

    #æ³¨é‡Šçš„è¯­å¥åˆ†ç±»
    classified_data = add_classification_property(data)
    print(f'Classification end. Data not saved')

    # æ³¨é‡Šçš„è¯­å¥çš„å…‹éš†æ£€æµ‹
    clone_detected_data = add_sentence_level_clone_type(classified_data)
    print(f'Clone detection end. Data not saved')

    print("Sentence Processing End.")
    return clone_detected_data



def change_label_use_heuristic_using_path(dataset_path):
    samples = read_json_file(dataset_path)
    changed_samples = change_label_use_heuristic(samples)
    return changed_samples

def change_label_use_heuristic(json_obj_list):
    '''
    å¦‚æœåŸæ¥çš„labelæ˜¯True(Positive, è¿‡æ—¶æ³¨é‡Š. è¯´æ˜ä¸¤ä¸ªæ³¨é‡Šä¸ç›¸ä¼¼, ä¸å­˜åœ¨æ³¨é‡Šå…‹éš†), ä½†æ˜¯æ£€æŸ¥å‡ºæ¥property==True(å³è¯¥æ³¨é‡Šçš„propertyç±»å‹å­˜åœ¨å…‹éš†)ğŸ‘clone_type = â€œlexicalâ€, é‚£ä¹ˆå°±ç¿»è½¬æ³¨é‡Šçš„labelä¸ºFalse, è®¤ä¸ºå­˜åœ¨æ³¨é‡Šå…‹éš†.
    å¯¹"how-it-is-done"å’Œ"why"åŒç†, åªæ˜¯æ¦‚ç‡å°ä¸€äº›

    "clone_type":
    {
        "textual": 0
        "lexical": 1
        "syntactic": 0
        "semantic": 0
    }

    Args:
        json_obj_list:

    Returns:

    '''
    json_obj_list = make_samples_balance(json_obj_list)



    # reversed_labels_cnt = 0 # è¢«å¯å‘å¼è§„åˆ™æ›´æ”¹labelä¸ºFalseçš„æ³¨é‡Šçš„æ•°é‡
    # for json_obj in json_obj_list:
    #     clone_type = json_obj['clone_type']
    #     classification = json_obj['classification']
    #     label = json_obj['label']
    #
    #
    #     if classification['property'] == True and clone_type['lexical'] == 1:
    #         label = False
    #         reversed_labels_cnt += 1
    #     elif classification["how-it-is-done"] == True or classification["why"] == True:
    #         random_num = random.random()
    #
    #         # æŒ‡å®šæ‰§è¡Œå‡½æ•°çš„æ¦‚ç‡
    #         probability = 0.01  # 1%çš„æ¦‚ç‡æ‰§è¡Œå‡½æ•°
    #
    #         # å¦‚æœéšæœºæ•°å°äºç­‰äºæ¦‚ç‡ï¼Œåˆ™æ‰§è¡Œå‡½æ•°
    #         if random_num <= probability:
    #             label = False
    #             reversed_labels_cnt += 1
    #     json_obj['label'] = label
    #
    # input_sample_num = len(json_obj_list)
    # mutation_rate = reversed_labels_cnt/input_sample_num
    # print(f'Original sample num: {input_sample_num}')
    # print(f"è¢«å¯å‘å¼è§„åˆ™æ›´æ”¹labelä¸ºFalseçš„æ³¨é‡Šçš„æ•°é‡: {reversed_labels_cnt}")
    # print(f"Mutation rate: {mutation_rate}")
    return json_obj_list


def make_samples_balance(json_obj_list):
    '''
    äººå·¥å¯¹æ•°æ®é›†æ‰“æ ‡ç­¾ï¼Œ è¯¥æ–¹æ³•åªæ˜¯ä½œä¸ºä¸€ä¸ªmock, å®é™…çš„è¿‡ç¨‹æ˜¯æ‰‹å·¥å®Œæˆçš„
    Args:
        json_obj_list:

    Returns:

    '''
    # æŒ‡å®šæ‰§è¡Œå‡½æ•°çš„æ¦‚ç‡
    probability = 0.005  # 0.01%çš„æ¦‚ç‡æ‰§è¡Œå‡½æ•°
    reversed_labels_cnt = 0 # è¢«éšæœºè§„åˆ™æ›´æ”¹labelä¸ºFalseçš„æ³¨é‡Šçš„æ•°é‡

    for json_obj in json_obj_list:
        if json_obj['label'] == False:
            random_num = random.random()
            # å¦‚æœéšæœºæ•°å°äºç­‰äºæ¦‚ç‡ï¼Œåˆ™æ‰§è¡Œå‡½æ•°
            if random_num <= probability:
                json_obj['label'] = True
                reversed_labels_cnt += 1

    input_sample_num = len(json_obj_list)
    reverse_rate = reversed_labels_cnt / input_sample_num
    print(f'Original sample num: {input_sample_num}')
    print(f"è¢«å¯å‘å¼è§„åˆ™æ›´æ”¹labelä¸ºTrueçš„æ³¨é‡Šçš„æ•°é‡: {reversed_labels_cnt}")
    print(f"Reverse rate: {reverse_rate}")
    return json_obj_list






def add_label(sample_data_file_path,  labeled_file_path, label_csv_file_path = ''):


    samples = read_json_file(sample_data_file_path)

    # è¯»å–å­˜å‚¨äº†äººå·¥æ ‡æ³¨ç»“æœçš„csvæ–‡ä»¶, å¯¹æ ·æœ¬è¿›è¡Œäººå·¥æ ‡æ³¨ã€‚è¯¥æ–¹æ³•åœ¨debugé˜¶æ®µä¸è¢«é‡‡ç”¨ã€‚
    write_label_to_dataset(label_csv_file_path, samples, labeled_file_path)


    # changed_samples = change_label_use_heuristic(samples)
    # save_jsonl_file(changed_samples, labeled_file_path)

    print(f'Manually Label Result is saved to the dataset. Save data to {labeled_file_path}')




# def change_label_use_heuristic_and_save_using_data(samples, changed_file_path):
#     '''
#     ä»¥æ•°æ®è€Œéæ•°æ®çš„è·¯å¾„ä½œä¸ºè¾“å…¥
#     Args:
#         samples:
#         changed_file_path:
#
#     Returns:
#
#     '''
#     changed_samples = change_label_use_heuristic(samples)
#     save_jsonl_file(changed_samples, changed_file_path)
#
#     print(f'Comment Label Reversion End. Save data to {changed_file_path}')


def aggregate_comments(sample_data_file_path):
    samples = read_json_file(sample_data_file_path)
    comments = merge_json_objects(samples)
    print(f'Comment Aggregation End. Data not saved')

    print(f'original sample num: {len(samples)}')
    print(f'aggregated comment num: {len(comments)}')
    return comments



def merge_samples_and_save(sample_data_file_path, merged_file_path):
    samples = read_json_file(sample_data_file_path)
    merge_samples_and_save_using_data(samples,merged_file_path)
    return

def merge_samples_and_save_using_data(samples, merged_file_path):
    merged_samples = merge_json_objects(samples)

    save_jsonl_file(merged_samples, merged_file_path)

    print(f'Dataset Merge End. Save data to {merged_file_path}')
    print(f'Original sample num: {len(samples)}')
    print(f'Merged sample num: {len(merged_samples)}')

    return

def merge_json_objects(json_obj_list):
    merged_objects = []
    object_dict = {}

    for obj in json_obj_list:
        idx = obj["idx"]
        sample_id = obj["sample_id"]

        # æ„å»ºåˆå¹¶åçš„å¯¹è±¡
        if idx in object_dict and sample_id in object_dict[idx]:
            # åˆå¹¶ src_desc å’Œ src_desc_tokens å±æ€§
            merged_obj = object_dict[idx][sample_id]
            merged_obj["src_desc"] += obj["src_desc"]
            merged_obj["src_desc_tokens"] += obj["src_desc_tokens"]

            # åŠ å…¥"composite_true_label_num"å±æ€§ï¼Œ ç»Ÿè®¡è¯¥åˆæˆçš„è¯„è®ºç©¶ç«Ÿç”±å‡ ä¸ªNegativeæ ·æœ¬(å³å…·å¤‡ç¬¬äºŒç±» CUP Comment cloneçš„æ ·æœ¬)ç»„æˆã€‚
            # Commentçš„"label"å€¼å–ç»„æˆçš„æ ·æœ¬ä¸­æ•°é‡è¾ƒå¤šçš„é‚£ä¸ªlabel. å³ï¼š å¦‚æœç»„æˆçš„æ ·æœ¬ä¸­å¤§å¤šæ•°éƒ½æ˜¯negative(False), åˆ™è¯¥åˆæˆçš„commentå°±æ˜¯negative(False)çš„
            if not merged_obj.has_key("composite_false_label_num"):
                merged_obj["composite_false_label_num"] = 0
            if not merged_obj.has_key("total_label_num"):
                merged_obj["total_label_num"] = 0

            merged_obj["total_label_num"] += 1
            if obj["label"] == False:
                merged_obj["composite_false_label_num"] += 1
            false_ratio = merged_obj["composite_false_label_num"] / merged_obj["total_label_num"]
            if false_ratio >= 0.5:
                merged_obj["label"] = False
            else:
                merged_obj["label"] = True
        else:
            merged_obj = obj

        # æ›´æ–°å­—å…¸ä¸­çš„å¯¹è±¡
        if idx not in object_dict:
            object_dict[idx] = {}
        object_dict[idx][sample_id] = merged_obj

    # å°†åˆå¹¶åçš„å¯¹è±¡æ·»åŠ åˆ°åˆ—è¡¨ä¸­
    for idx_dict in object_dict.values():
        merged_objects.extend(list(idx_dict.values()))

    return merged_objects

def download_file(download_data_path, save_file_path, line_count = 100):
    data = read_json_file_for_given_line(download_data_path, line_count)
    save_jsonl_file(data, save_file_path)
    print(f"file download end. Downloaded {line_count} lines")

if __name__ == "__main__":
    pass
    # file_name = 'test'
    # input_file_dir = '../data'
    #
    # output_file_dir = '../data'
    #
    #
    # train_data_path = f'{input_file_dir}/{file_name}.jsonl'
    # classified_file_path = f'{output_file_dir}/{file_name}_classified.jsonl'
    # clone_detected_file_path = f'{output_file_dir}/{file_name}_detected.jsonl'
    # merged_file_path = f'{output_file_dir}/{file_name}_merged.jsonl'
    #
    # final_processed_file_path = clone_detected_file_path
    #
    #
    #
    # process_sentence(train_data_path, classified_file_path, clone_detected_file_path)
    # aggregate_comments_and_save(clone_detected_file_path, merged_file_path )
    #
    #
    # look_classification(merged_file_path)
